# SaaS Hunter üéØ

**Automated SaaS opportunity discovery system** ‚Äî monitors Reddit, Hacker News, and GitHub to surface validated pain points and unmet market needs.

Built to find profitable SaaS ideas by tracking real developer and entrepreneur complaints, with a focus on signal over noise.

---

## üöÄ What It Does

SaaS Hunter runs on cron, collecting opportunities from:

- **Reddit** ‚Äî r/SaaS, r/startups, r/Entrepreneur, r/smallbusiness, r/sales (every 3h)
- **Hacker News** ‚Äî Show HN, Ask HN, trending discussions (every 4h)
- **GitHub** ‚Äî Trending repos, highly-reacted issues (daily)

Each opportunity is **scored 0-100** based on:
- Source credibility
- Engagement (upvotes, comments, reactions)
- Pain point clarity ("sick of", "frustrated")
- Specificity (detailed problems > vague complaints)
- Freshness (recent = higher score)
- Niche fit (B2B SaaS, developer tools)

**Daily digest** delivered via Telegram at 8 AM UTC with top opportunities ranked and categorized.

---

## üìä Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COLLECTION (Cron)                                           ‚îÇ
‚îÇ  Reddit (3h) ‚Üí HN (4h) ‚Üí GitHub (daily)                    ‚îÇ
‚îÇ  Output: data/raw/SOURCE_YYYYMMDD_HHMMSS.json              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PROCESSING (Every 6h)                                       ‚îÇ
‚îÇ  1. Load new raw files                                      ‚îÇ
‚îÇ  2. Score each opportunity (0-100)                          ‚îÇ
‚îÇ  3. Deduplicate across sources                              ‚îÇ
‚îÇ  4. Enrich with metadata                                    ‚îÇ
‚îÇ  Output: data/processed/opportunities_YYYYMMDD.jsonl        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ AGGREGATION (Daily 8 AM)                                    ‚îÇ
‚îÇ  1. Load last 24h opportunities                             ‚îÇ
‚îÇ  2. Rank by score                                           ‚îÇ
‚îÇ  3. Group by domain                                         ‚îÇ
‚îÇ  4. Generate digest                                         ‚îÇ
‚îÇ  Output: data/digests/digest_YYYYMMDD.md                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DELIVERY                                                     ‚îÇ
‚îÇ  Telegram bot ‚Üí top 3-5 opportunities                       ‚îÇ
‚îÇ  (via OpenClaw heartbeat polling)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Setup

### Prerequisites
- Python 3.8+
- Reddit API credentials (free)
- GitHub personal access token (free)

### Installation

1. **Clone the repo:**
   ```bash
   git clone https://github.com/Alexeyisme/saas-hunter.git
   cd saas-hunter
   ```

2. **Create virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

3. **Configure API credentials:**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

   Required:
   - `REDDIT_CLIENT_ID` / `REDDIT_CLIENT_SECRET` ‚Äî [Get here](https://www.reddit.com/prefs/apps)
   - `GITHUB_TOKEN` ‚Äî [Create token](https://github.com/settings/tokens)

   Optional (Hacker News is public, no auth needed):
   - `TELEGRAM_BOT_TOKEN` / `TELEGRAM_CHAT_ID` ‚Äî for delivery

4. **Test collectors:**
   ```bash
   cd scripts
   python3 reddit_monitor.py
   python3 hackernews_monitor.py
   python3 github_monitor.py
   ```

5. **Set up cron jobs:**
   ```bash
   crontab -e
   ```

   Add:
   ```cron
   # Reddit - every 3 hours
   5 */3 * * * cd /path/to/saas-hunter/scripts && /path/to/venv/bin/python3 reddit_monitor.py >> /path/to/saas-hunter/logs/cron_reddit.log 2>&1

   # HackerNews - every 4 hours
   15 */4 * * * cd /path/to/saas-hunter/scripts && /path/to/venv/bin/python3 hackernews_monitor.py >> /path/to/saas-hunter/logs/cron_hn.log 2>&1

   # GitHub - daily at 6 AM
   0 6 * * * cd /path/to/saas-hunter/scripts && /path/to/venv/bin/python3 github_monitor.py >> /path/to/saas-hunter/logs/cron_github.log 2>&1

   # Process - every 6 hours
   25 */6 * * * cd /path/to/saas-hunter/scripts && /path/to/venv/bin/python3 process_opportunities.py >> /path/to/saas-hunter/logs/cron_process.log 2>&1

   # Digest - daily at 8 AM
   0 8 * * * cd /path/to/saas-hunter/scripts && /path/to/venv/bin/python3 generate_digest.py >> /path/to/saas-hunter/logs/cron_digest.log 2>&1
   ```

---

## üìÅ Project Structure

```
saas-hunter/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ reddit_monitor.py          # Reddit collector
‚îÇ   ‚îú‚îÄ‚îÄ hackernews_monitor.py      # HN collector
‚îÇ   ‚îú‚îÄ‚îÄ github_monitor.py          # GitHub collector
‚îÇ   ‚îú‚îÄ‚îÄ process_opportunities.py   # Scoring + deduplication
‚îÇ   ‚îú‚îÄ‚îÄ generate_digest.py         # Daily aggregation
‚îÇ   ‚îú‚îÄ‚îÄ send_telegram_openclaw.py  # Telegram delivery
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Shared configuration
‚îÇ   ‚îú‚îÄ‚îÄ scoring.py                 # Scoring algorithm
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Helper functions
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Collected JSON files
‚îÇ   ‚îú‚îÄ‚îÄ processed/                 # Scored opportunities (JSONL)
‚îÇ   ‚îú‚îÄ‚îÄ digests/                   # Daily markdown summaries
‚îÇ   ‚îú‚îÄ‚îÄ telegram_outbox/           # Pending Telegram messages
‚îÇ   ‚îî‚îÄ‚îÄ seen_ids.json              # Deduplication tracking
‚îú‚îÄ‚îÄ logs/                          # Cron execution logs
‚îú‚îÄ‚îÄ scoring_config.json            # Scoring weights/thresholds
‚îú‚îÄ‚îÄ .env                           # API credentials (git-ignored)
‚îú‚îÄ‚îÄ .env.example                   # Template for credentials
‚îî‚îÄ‚îÄ README.md                      # This file
```

---

## üéØ Scoring Algorithm

Each opportunity gets a score from **0-100 points**:

| **Factor**               | **Max Points** | **Description**                                  |
|--------------------------|----------------|--------------------------------------------------|
| **Source Credibility**   | 20             | GitHub > HN > Reddit                             |
| **Engagement**           | 25             | Upvotes, comments, reactions                     |
| **Pain Point Clarity**   | 20             | Keywords: "sick of", "frustrated", "hate"        |
| **Specificity**          | 15             | Detailed posts, metrics, numbers                 |
| **Freshness**            | 10             | <6h = 10 pts, <24h = 7 pts, <72h = 4 pts         |
| **Niche Fit**            | 10             | B2B, SaaS, API, dev tools                        |

**Example High-Score Opportunity (92 pts):**
```
Title: "Sick of paying $40/month for Supademo just to make ONE demo"
Source: Reddit r/SaaS
Engagement: 15 upvotes, 8 comments
Pain Signal: "sick of", pricing complaint, specific competitor
Niche: SaaS product demos
```

Configuration: `scoring_config.json`

---

## üìÆ Daily Digest Format

```markdown
üéØ SaaS Opportunities ‚Äî Feb 15, 2026

1. ‚≠êÔ∏è Alternative to Supademo (92 pts)
   üìç reddit:SaaS
   üîó https://reddit.com/r/SaaS/...

2. ‚≠êÔ∏è Workflow Automation for Non-Developers (85 pts)
   üìç reddit:nocode
   üîó https://reddit.com/r/nocode/...

3. üí° Email Management AI (72 pts)
   üìç reddit:SaaS
   üîó https://reddit.com/r/SaaS/...

üìä 39 collected | 2 high quality (60+)
```

Delivered via Telegram to your phone every morning.

---

## üîß Configuration

### Tuning the Scoring

Edit `scoring_config.json`:

```json
{
  "pain_keywords": ["sick of", "frustrated", "hate", "tired of"],
  "pay_keywords": ["would pay", "expensive", "pricing"],
  "min_score_threshold": 40,
  "freshness_hours": {
    "high": 6,
    "medium": 24,
    "low": 72
  }
}
```

### Subreddit Filters

Edit `scripts/config.py`:

```python
SUBREDDITS = [
    'SaaS',
    'startups',
    'Entrepreneur',
    'smallbusiness',
    'sales'
]
```

### GitHub Topics

Edit `scripts/config.py`:

```python
GITHUB_TOPICS = [
    'saas',
    'productivity',
    'developer-tools',
    'automation'
]
```

---

## üìä Monitoring

Check collection health:

```bash
# View recent logs
tail -f logs/cron_reddit.log
tail -f logs/cron_process.log

# Check last digest
cat data/digests/digest_$(date +%Y%m%d).md

# Count today's opportunities
wc -l data/processed/opportunities_$(date +%Y%m%d).jsonl
```

---

## üí∞ Cost

**Current setup: $0/month**

- Reddit API: Free tier (60 requests/min)
- GitHub API: Free tier (5,000 requests/hour)
- Hacker News: Public RSS, no auth
- Storage: ~5 MB/day (~150 MB/month)
- Compute: Runs on your server/VPS

Designed to fit within a **$15/month** budget if you add paid features later (e.g., OpenAI for clustering).

---

## üìà Example Output

**Real digest from Feb 15, 2026:**

- **39 opportunities** collected
- **2 high-quality** (60+ score)
- **10 worth exploring** (40-59 score)

Top find:
> "6 years in sales, moving to SF in 5 months. How would you approach this?" (65.8 pts)  
> Signal: Career transition pain point, location-specific networking needs

---

## ü§ù Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md).

**Areas for improvement:**
- Better deduplication (ML-based clustering)
- Domain classification (auto-categorize by industry)
- Sentiment analysis (detect urgency/willingness to pay)
- Web dashboard for browsing opportunities

---

## üìú License

MIT License - see [LICENSE](LICENSE)

---

## üôè Acknowledgments

- Built with [PRAW](https://praw.readthedocs.io/) for Reddit
- [feedparser](https://pypi.org/project/feedparser/) for Hacker News RSS
- [PyGithub](https://pygithub.readthedocs.io/) for GitHub API

---

## üìß Contact

- **Author:** Alexey
- **GitHub:** [@Alexeyisme](https://github.com/Alexeyisme)
- **Project:** [saas-hunter](https://github.com/Alexeyisme/saas-hunter)

---

**Built to help developers find profitable SaaS ideas by listening to what people actually need.**

*Last updated: Feb 15, 2026*
